{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "06fa5574-30f2-44da-bc91-e4aeff4baec3",
    "_uuid": "150a3ec7-5601-430f-9f41-e73648b8bfdf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-27T02:42:39.051871Z",
     "iopub.status.busy": "2025-08-27T02:42:39.051673Z",
     "iopub.status.idle": "2025-08-27T02:43:25.083728Z",
     "shell.execute_reply": "2025-08-27T02:43:25.082933Z",
     "shell.execute_reply.started": "2025-08-27T02:42:39.051854Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q py_vncorenlp\n",
    "\n",
    "import py_vncorenlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\",\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7d10865f-c20d-491c-b65c-559060aabaeb",
    "_uuid": "7abb613a-3e13-4691-b7db-ff443193880f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-27T02:43:49.034547Z",
     "iopub.status.busy": "2025-08-27T02:43:49.034285Z",
     "iopub.status.idle": "2025-08-27T02:43:54.808413Z",
     "shell.execute_reply": "2025-08-27T02:43:54.807559Z",
     "shell.execute_reply.started": "2025-08-27T02:43:49.034523Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"gambling\": 0,\n",
    "    \"movies\": 1,\n",
    "    \"ecommerce\": 2,\n",
    "    \"government\": 3,\n",
    "    \"education\": 4,\n",
    "    \"technology\": 5,\n",
    "    \"tourism\": 6,\n",
    "    \"health\": 7,\n",
    "    \"finance\": 8,\n",
    "    \"media\": 9,\n",
    "    \"nonprofit\": 10,\n",
    "    \"realestate\": 11,\n",
    "    \"services\": 12,\n",
    "    \"industries\": 13,\n",
    "    \"agriculture\": 14\n",
    "}\n",
    "\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model_name = \"vinai/phobert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "model.to(device)\n",
    "print(f\"Model '{model_name}' and tokenizer loaded successfully with {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "813e5345-0b9b-476f-9d3c-7c5d304a075d",
    "_uuid": "063cb694-00e2-44ec-af37-05ae92c56d67",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure the directory exists before downloading\n",
    "os.makedirs('/kaggle/working/vncorenlp', exist_ok=True)\n",
    "\n",
    "# Download VnCoreNLP model\n",
    "py_vncorenlp.download_model(save_dir='/kaggle/working/vncorenlp')\n",
    "\n",
    "# Load the segmenter\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(\n",
    "    annotators=[\"wseg\"], \n",
    "    save_dir='/kaggle/working/vncorenlp'\n",
    ")\n",
    "\n",
    "# Test word segmentation\n",
    "text = \"√îng Nguy·ªÖn Kh·∫Øc Ch√∫c ƒëang l√†m vi·ªác t·∫°i ƒê·∫°i h·ªçc Qu·ªëc gia H√† N·ªôi. B√† Lan, v·ª£ √¥ng Ch√∫c, c≈©ng l√†m vi·ªác t·∫°i ƒë√¢y.\"\n",
    "output = rdrsegmenter.word_segment(text)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e8db8bd-8697-464e-9237-db63ff2e9838",
    "_uuid": "2f8f9ae0-f7ff-482f-a354-2abf865c64d9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-27T02:56:32.076672Z",
     "iopub.status.busy": "2025-08-27T02:56:32.075873Z",
     "iopub.status.idle": "2025-08-27T02:57:48.812275Z",
     "shell.execute_reply": "2025-08-27T02:57:48.811357Z",
     "shell.execute_reply.started": "2025-08-27T02:56:32.076649Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='insert-path')\n",
    "dataset = dataset['train']\n",
    "print('dataset loaded!')\n",
    "\n",
    "def encode_labels(examples):\n",
    "    examples['label'] = [label2id[label] for label in examples['label']]\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(encode_labels, batched=True)\n",
    "print(\"Labels encoded to numerical IDs.\")\n",
    "\n",
    "token_lengths = []\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Apply Vietnamese word segmentation\n",
    "    segmented_texts = [' '.join(rdrsegmenter.word_segment(text)) for text in examples['text']]\n",
    "    \n",
    "    # Tokenize segmented text\n",
    "    tokenized_inputs = tokenizer(\n",
    "        segmented_texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=64\n",
    "    )\n",
    "    \n",
    "    # Count non-padding tokens\n",
    "    for input_ids in tokenized_inputs['input_ids']:\n",
    "        length = len([token_id for token_id in input_ids if token_id != tokenizer.pad_token_id])\n",
    "        token_lengths.append(length)\n",
    "    \n",
    "    return tokenized_inputs\n",
    "    \n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, load_from_cache_file=False)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")    # Hugging Face requires a specific column named 'labels'\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "features = tokenized_dataset\n",
    "labels = [label.item() for label in tokenized_dataset['labels']]\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(labels)),\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "train_dataset = features.select(train_indices)\n",
    "eval_dataset = features.select(test_indices)\n",
    "\n",
    "\n",
    "max_length = max(token_lengths)\n",
    "min_length = min(token_lengths)\n",
    "avg_length = sum(token_lengths) / len(token_lengths)\n",
    "\n",
    "print(f\"Token length stats before padding:\")\n",
    "print(f\"  üîπ Max length: {max_length}\")\n",
    "print(f\"  üîπ Min length: {min_length}\")\n",
    "print(f\"  üîπ Avg length: {avg_length:.2f} tokens\")\n",
    "print(f\"Dataset tokenized and split into {len(train_dataset)} training examples and {len(eval_dataset)} evaluation examples.\")\n",
    "\n",
    "labels = [label.item() for label in train_dataset['labels']]\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Emphasize \n",
    "# scale_factor = 1.0\n",
    "# class_weights[0] *= scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1db54a70-97f2-473b-946c-119bb043c1ec",
    "_uuid": "97b9872d-44e8-4ef3-93c3-00f0430349d5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-27T02:59:37.044519Z",
     "iopub.status.busy": "2025-08-27T02:59:37.043658Z",
     "iopub.status.idle": "2025-08-27T02:59:37.433336Z",
     "shell.execute_reply": "2025-08-27T02:59:37.432688Z",
     "shell.execute_reply.started": "2025-08-27T02:59:37.044493Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot histogram of token length distribution (after truncation)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(token_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Token Length Distribution (Before Padding)\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.axvline(x=64, color='red', linestyle='--', label='max_length=64')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d25f6b47-0d61-4a7b-8733-d85ff6a0a841",
    "_uuid": "39bcc22a-6a9b-47d9-8890-7332a8f3d4fd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # After finish evaluation, re-train on the whole dataset to maximise learning (intentional data leakage)\n",
    "# train_dataset = tokenized_dataset\n",
    "# print(f\"Training on the entire dataset with {len(train_dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "920ed1e7-62dc-433e-9760-990dd0a723d3",
    "_uuid": "9469ea0f-b491-414c-bd17-24a2558f2e6c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-27T03:00:03.169666Z",
     "iopub.status.busy": "2025-08-27T03:00:03.169379Z",
     "iopub.status.idle": "2025-08-27T03:00:03.177092Z",
     "shell.execute_reply": "2025-08-27T03:00:03.176403Z",
     "shell.execute_reply.started": "2025-08-27T03:00:03.169646Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, class_weights=None, gamma=2.0, reduction='mean', label_smoothing=0.2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        num_classes = logits.size(1)\n",
    "        smoothed_labels = F.one_hot(targets, num_classes).float()\n",
    "        smoothed_labels = smoothed_labels * (1 - self.label_smoothing) + self.label_smoothing / num_classes\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        ce_loss = -(smoothed_labels * log_probs).sum(dim=1)\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights[targets]\n",
    "            ce_loss = ce_loss * weights\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, focal_loss=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss = focal_loss\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.focal_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a457f466-ebc4-481d-893a-18bd4a8db6b8",
    "_uuid": "b9685e7f-c197-4c5b-b246-82e65a19c17c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-27T03:00:19.090886Z",
     "iopub.status.busy": "2025-08-27T03:00:19.090568Z",
     "iopub.status.idle": "2025-08-27T03:00:19.130578Z",
     "shell.execute_reply": "2025-08-27T03:00:19.129700Z",
     "shell.execute_reply.started": "2025-08-27T03:00:19.090865Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "focal_loss = FocalLoss(class_weights=class_weights, gamma=2.0, label_smoothing=0.2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, average='weighted'),\n",
    "        \"recall\": recall_score(labels, preds, average='weighted'),\n",
    "        \"f1\": f1_score(labels, preds, average='weighted')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e1f2a265-5d91-4dab-a005-99d01fe5dcbf",
    "_kg_hide-input": true,
    "_uuid": "6eb4a6b7-d551-461c-ba90-1dc77d588f0e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Reload model if retraining\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "33014249-c2b7-4480-a3f5-9a84ce0823ad",
    "_uuid": "e47c663c-6269-4702-9cef-00fd9dab7262",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-27T03:00:22.432945Z",
     "iopub.status.busy": "2025-08-27T03:00:22.432654Z",
     "iopub.status.idle": "2025-08-27T03:00:22.451083Z",
     "shell.execute_reply": "2025-08-27T03:00:22.450169Z",
     "shell.execute_reply.started": "2025-08-27T03:00:22.432924Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    focal_loss=focal_loss,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.001)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "55abe774-75f5-4ffd-a604-22b8a38defb7",
    "_uuid": "2292a286-b900-4bd4-b10d-24f0d4ea801d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-27T03:00:24.898932Z",
     "iopub.status.busy": "2025-08-27T03:00:24.898642Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f3283565-0c37-406a-8ab0-0cf866d5a5c6",
    "_uuid": "64b682ad-a9bd-4fe1-a9bc-52a98a63eb73",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-06T06:45:57.362352Z",
     "iopub.status.busy": "2025-08-06T06:45:57.362065Z",
     "iopub.status.idle": "2025-08-06T06:45:58.608588Z",
     "shell.execute_reply": "2025-08-06T06:45:58.607762Z",
     "shell.execute_reply.started": "2025-08-06T06:45:57.36233Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_save_path = \"./fine_tuned_phobert\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Fine-tuned model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c713a1d7-74b2-4771-845f-f6858b60c1e0",
    "_uuid": "0e913875-443f-4045-bf83-04fd8d8bd19a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Empty the working directory if retraining\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# working_dir = \"/kaggle/working/\"\n",
    "# print(f\"Attempting to clear the directory: {working_dir}\")\n",
    "\n",
    "# if os.path.exists(working_dir):\n",
    "#     for item in os.listdir(working_dir):\n",
    "#         item_path = os.path.join(working_dir, item)\n",
    "#         try:\n",
    "#             if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "#                 os.unlink(item_path)\n",
    "#             elif os.path.isdir(item_path):\n",
    "#                 shutil.rmtree(item_path)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error removing {item_path}: {e}\")\n",
    "#     print(f\"Contents of {working_dir} cleared.\")\n",
    "# else:\n",
    "#     print(f\"Directory {working_dir} does not exist.\")\n",
    "\n",
    "# if not os.listdir(working_dir):\n",
    "#     print(f\"Directory {working_dir} is now empty.\")\n",
    "# else:\n",
    "#     print(f\"Directory {working_dir} still contains items: {os.listdir(working_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43c93d38-8f45-440e-8bdf-ea6f3d14dc93",
    "_uuid": "ea60e810-de6b-401d-969d-65f6d099aa76",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-30T14:37:36.17871Z",
     "iopub.status.busy": "2025-07-30T14:37:36.177925Z",
     "iopub.status.idle": "2025-07-30T14:37:52.775767Z",
     "shell.execute_reply": "2025-07-30T14:37:52.774612Z",
     "shell.execute_reply.started": "2025-07-30T14:37:36.178672Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the test set\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "96fb86d7-3a3d-47b6-8f43-106788702a6f",
    "_uuid": "7a881c39-721b-4058-a372-b008afade54b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Plot the graphs\n",
    "# log_history = trainer.state.log_history\n",
    "\n",
    "# epochs = []\n",
    "# train_losses = []\n",
    "# eval_losses = []\n",
    "# eval_accuracies = []\n",
    "# eval_f1_scores = []\n",
    "# eval_precisions = []\n",
    "# eval_recalls = []\n",
    "\n",
    "# for log in log_history:\n",
    "#     if 'loss' in log and 'learning_rate' in log and 'epoch' in log:\n",
    "#         train_losses.append(log['loss'])\n",
    "#         epochs.append(log['epoch'])\n",
    "#     elif 'eval_loss' in log:\n",
    "#         eval_losses.append(log['eval_loss'])\n",
    "#         eval_accuracies.append(log['eval_accuracy'])\n",
    "#         eval_f1_scores.append(log['eval_f1'])\n",
    "#         eval_precisions.append(log['eval_precision'])\n",
    "#         eval_recalls.append(log['eval_recall'])\n",
    "\n",
    "# eval_epochs = [log['epoch'] for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "# # Plotting Loss\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(epochs[:len(train_losses)], train_losses, label='Training Loss')\n",
    "# plt.plot(eval_epochs, eval_losses, label='Validation Loss')\n",
    "# plt.title('Loss over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Plotting Accuracy and F1-score\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(eval_epochs, eval_accuracies, label='Validation Accuracy')\n",
    "# plt.plot(eval_epochs, eval_f1_scores, label='Validation F1-score (Weighted)')\n",
    "# plt.plot(eval_epochs, eval_precisions, label='Validation Precision (Weighted)')\n",
    "# plt.plot(eval_epochs, eval_recalls, label='Validation Recall (Weighted)')\n",
    "# plt.title('Metrics over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Score')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout() \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b76173c1-f4b7-4736-a036-2798fc5fa07a",
    "_uuid": "83de6af2-4e12-456c-b961-fa2cfea7edd8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T03:17:49.605584Z",
     "iopub.status.busy": "2025-08-07T03:17:49.604791Z",
     "iopub.status.idle": "2025-08-07T03:18:16.053427Z",
     "shell.execute_reply": "2025-08-07T03:18:16.052569Z",
     "shell.execute_reply.started": "2025-08-07T03:17:49.605547Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save misclassified samples to CSV\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get predictions on eval_dataset\n",
    "predictions_output = trainer.predict(eval_dataset)\n",
    "\n",
    "# Convert logits to predicted class and confidence\n",
    "logits = predictions_output.predictions\n",
    "probs = torch.softmax(torch.tensor(logits), dim=1)\n",
    "predicted_labels = torch.argmax(probs, axis=1)\n",
    "confidences = torch.max(probs, axis=1).values\n",
    "\n",
    "# Get ground truth labels\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "# Get original text from dataset\n",
    "original_texts = [tokenizer.decode(inputs['input_ids'], skip_special_tokens=True) for inputs in eval_dataset]\n",
    "\n",
    "# Map IDs to label names\n",
    "true_label_names = [id2label[label] for label in true_labels]\n",
    "predicted_label_names = [id2label[label.item()] for label in predicted_labels]\n",
    "\n",
    "# Filter misclassified samples\n",
    "misclassified = []\n",
    "for text, true, pred, conf in zip(original_texts, true_label_names, predicted_label_names, confidences):\n",
    "    if true != pred:\n",
    "        misclassified.append({\n",
    "            \"original_text\": text,\n",
    "            \"true_label\": true,\n",
    "            \"predicted_label\": pred,\n",
    "            \"confidence\": round(conf.item(), 4)\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df_misclassified = pd.DataFrame(misclassified)\n",
    "df_misclassified.to_csv(\"misclassified_samples.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Saved {len(df_misclassified)} misclassified samples to misclassified_samples.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "497637b8-eded-4c31-9f18-d07a9409866b",
    "_uuid": "dc3b7647-9ce5-4e9d-9f17-269e4591e69f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fcf03ebc-ae6e-4eeb-896a-3144d1ca7635",
    "_uuid": "84186efe-0770-4b55-bd90-3eff769c8a43",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T03:45:42.551741Z",
     "iopub.status.busy": "2025-08-07T03:45:42.550917Z",
     "iopub.status.idle": "2025-08-07T03:45:42.557825Z",
     "shell.execute_reply": "2025-08-07T03:45:42.556865Z",
     "shell.execute_reply.started": "2025-08-07T03:45:42.551707Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    #text = text.lower()\n",
    "\n",
    "    # === Preserve domain dots, decimal dots, and URL hyphens ===\n",
    "    text = re.sub(r'(\\w)\\.(?=\\w)', r'\\1<DOMAIN>', text)      # domain dots\n",
    "    text = re.sub(r'(\\d)\\.(?=\\d)', r'\\1<DECIMAL>', text)     # decimal dots\n",
    "    text = re.sub(r'(\\w)-(?=\\w)', r'\\1<HYPHEN>', text)       # hyphen inside words/domains\n",
    "\n",
    "    # === Remove remaining dots and hyphens ===\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace('-', '')\n",
    "\n",
    "    # === Replace one or more underscores with a single space ===\n",
    "    text = re.sub(r'_+', ' ', text)\n",
    "\n",
    "    # === Restore preserved characters ===\n",
    "    text = text.replace('<DOMAIN>', '.')\n",
    "    text = text.replace('<DECIMAL>', '.')\n",
    "    text = text.replace('<HYPHEN>', '-')\n",
    "\n",
    "    # === Handle commas ===\n",
    "    text = re.sub(r'(?<=[a-z0-9]),(?=[a-z])', ' ', text)  # digit/letter ‚Üí letter\n",
    "    text = re.sub(r'(?<=[a-z]),(?=[0-9])', ' ', text)     # letter ‚Üí digit\n",
    "    text = re.sub(r',(?=\\D)|(?<=\\D),', '', text)          # remove other commas\n",
    "\n",
    "    # === Remove unwanted punctuation (keep quotes, %, /) ===\n",
    "    text = re.sub(r'[^\\w\\s\\.,/%\"]', '', text)\n",
    "\n",
    "    # === Normalize spaces ===\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b24a8230-b8d7-48fc-aebc-0a549c4f17ef",
    "_uuid": "60ec5431-244c-4672-9b7e-722602ca68c5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T03:46:21.286007Z",
     "iopub.status.busy": "2025-08-07T03:46:21.285728Z",
     "iopub.status.idle": "2025-08-07T03:46:22.399113Z",
     "shell.execute_reply": "2025-08-07T03:46:22.398405Z",
     "shell.execute_reply.started": "2025-08-07T03:46:21.285988Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Testing manually\n",
    "def predict_text_class(text_input: str):\n",
    "    # Clean the text first\n",
    "    # text = clean_text(text_input)\n",
    "\n",
    "    # Apply Vietnamese word segmentation (same as training)\n",
    "    segmented_text = ' '.join(rdrsegmenter.word_segment(text))\n",
    "\n",
    "    # Tokenize segmented text\n",
    "    inputs = tokenizer(\n",
    "        segmented_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "    # Move tensors to the correct device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_label = id2label[predicted_class_id]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "example_texts = [\n",
    "    \"H·ªá th·ªëng nha khoa T√¢m ƒê·ª©c Smile d·∫´n ƒë·∫ßu v·ªÅ c√°c d·ªãch v·ª• c·∫•y gh√©p Implant, rƒÉng s·ª© th·∫©m m·ªπ, ni·ªÅng rƒÉng uy t√≠n h√†ng ƒë·∫ßu, ∆∞u ƒë√£i l√™n ƒë·∫øn 60%.\",\n",
    "    \"Nh√† Xe M·ªπ Duy√™n - V·∫≠n chuy·ªÉn h√†nh kh√°ch v√† h√†ng h√≥a chuy√™n tuy·∫øn H·ªì Ch√≠ Minh ƒëi S√≥c TrƒÉng v√† ng∆∞·ª£c l·∫°i Nh·∫≠n v√© Nh·∫≠n m√£ v√©, x√°c nh·∫≠n v√† l√™n xe\",\n",
    "    \"C√¥ng ty t∆∞ v·∫•n du h·ªçc M·ªπ c·∫≠p nh·∫≠t m·ªõi nh·∫•t v·ªÅ ƒëi·ªÅu ki·ªán, chi ph√≠, h·ªì s∆° xin visa du h·ªçc m·ªπ, c∆° h·ªôi ƒë·ªãnh c∆∞. C√¥ng ty du h·ªçc √Å - √Çu 24 nƒÉm kinh nghi·ªám, ƒë·ªânh cao uy t√≠n v√† ch·∫•t l∆∞·ª£ng.\",\n",
    "    \"m√°y rung c·∫ßm tay t·∫∑ng b·∫°n g√°i\",\n",
    "    \"thd cybersecurity\",\n",
    "    \"Vay C·∫ßm C·ªë, Gi·∫£i Ng√¢n 15p T·ªïng h·ª£p c√°c b√†i vi·∫øt t·ª´ c∆° quan b√°o ch√≠ truy·ªÅn th√¥ng uy t√≠n, ph·∫£n √°nh ho·∫°t ƒë·ªông, th√†nh t·ª±u n·ªïi b·∫≠t v√† chi·∫øn l∆∞·ª£c ph√°t tri·ªÉn c·ªßa F88.\",\n",
    "    \"ƒê∆∞·ªùng d√¢y ƒë√°nh b·∫°c, n·ªó h·ªß, c√° ƒë·ªô b√≥ng ƒë√° r·ª≠a ti·ªÅn l√™n ƒë·∫øn 1.000 t·ª∑ ƒë·ªìng b·ªã tri·ªát ph√°\",\n",
    "    \"Samsung Galaxy Z Fold7 | Foldable meets Ultra Sleek | Samsung Australia\",\n",
    "    \"ƒë∆∞·ªùng v√†o tim em √¥i bƒÉng gi√°\",\n",
    "    \"Ngh·ªã ƒë·ªãnh s·ªë 46/2017/Nƒê-CP ng√†y 21/4/2017 c·ªßa Ch√≠nh ph·ªß quy ƒë·ªãnh v·ªÅ ho·∫°t ƒë·ªông ƒë·∫ßu t∆∞ gi√°o d·ª•c trong c√°c ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o m·∫ßm non, ph·ªï th√¥ng, ƒë·∫°i h·ªçc; ...\",\n",
    "    \"5 ng√†y tr∆∞·ªõc ‚Äî tachi,N·ªÅn t·∫£ng x·ªï s·ªë tr·ª±c tuy·∫øn v·ªõi t·ª∑ l·ªá hoa h·ªìng cho ƒë·∫°i l√Ω cao nh·∫•t. H·ª£p t√°c v√† ki·∫øm thu nh·∫≠p th·ª• ƒë·ªông c√πng ch√∫ng t√¥i.\",\n",
    "    \"Th√¥ng b√°o tuy·ªÉn sinh ƒëi h·ªçc t·∫°i CƒÉm-pu-chia di·ªán Hi·ªáp ƒë·ªãnh nƒÉm 2025 ¬∑ TH√îNG B√ÅO TUY·ªÇN SINH ƒêI H·ªåC T·∫†I MA-R·ªêC NƒÇM 2025 ¬∑ TH√îNG B√ÅO TUY·ªÇN SINH ƒêI H·ªåC T·∫†I M√îNG C·ªî ...\",\n",
    "    \"7 ng√†y tr∆∞·ªõc ‚Äî qq188bet! N·ªÅn t·∫£ng x·ªï s·ªë tr·ª±c tuy·∫øn v·ªõi t·ª∑ l·ªá hoa h·ªìng cho ƒë·∫°i l√Ω cao nh·∫•t. H·ª£p t√°c v√† ki·∫øm thu nh·∫≠p th·ª• ƒë·ªông c√πng ch√∫ng t√¥i. Tr·ªü th√†nh ƒë·ªëi t√°c ...\",\n",
    "    \"b·ªën ƒë√¥i th√¥ng ch·∫∑t ƒë∆∞·ª£c g√¨? c√°ch d√πng b·ªën ƒë√¥i th√¥ng hi·ªáu qu·∫£. 19 thg 10, 2024 - b·ªën ƒë√¥i th√¥ng l√† m·ªôt t·ªï h·ª£p b√†i ƒë·∫∑c bi·ªát trong c√°c tr√≤ ch∆°i b√†i mi·ªÅn nam, ƒë·∫∑c bi·ªát l√† ti·∫øn l√™n. khi m·ªôt ng∆∞·ªùi ch∆°i s·ªü h·ªØu b·ªën ƒë√¥i b√†i gi·ªëng nhau ...\",\n",
    "    \"ƒêƒÉng nh·∫≠p VIC\",\n",
    "    \"GGBet\",\n",
    "    \"K·∫øt qu·∫£ CCHC\",\n",
    "    \"S·∫ßu ri√™ng Ri6\",\n",
    "    \"V√∫ s·ªØa t√≠m\",\n",
    "    \"Tr√† C2\",\n",
    "    \"C∆°n_s·ªët_v√†ng_bit C∆°n_s·ªët_v√†ng_bit-ƒê∆∞·ª£c thi ƒë·∫•u tr√™n s√¢n nh√† ·ªü tr·∫≠n chung k·∫øt l∆∞·ª£t v·ªÅ ƒë·ªôi tuy·ªÉn Th√°i Lan nh·∫≠p cu·ªôc v·ªõi tinh th·∫ßn th·∫ßn r·∫•t cao. ƒê·ªôi ch·ªß s√¢n Rajamangala c·∫ßm\",\n",
    "    \"kwin congan.khanhhoa.gov.vn\",\n",
    "    \"nohu69 congan.khanhhoa.gov.vn\",\n",
    "    \"C∆°n_s·ªët_v√†ng_bit\",\n",
    "    \"+85 s·∫£n ph·∫©m s√†n g·ªó Malaysia si√™u ch·ªãu n∆∞·ªõc gi√° t·ªët nh·∫•t\",\n",
    "    \"S√†n g·ªó Malaysia ch√≠nh h√£ng, ch·∫•t l∆∞·ª£ng, gi√° c·∫£ c·∫°nh tranh ƒë∆∞·ª£c ph√¢n ph·ªëi b·ªüi JANHOME l√† h·ªá th·ªëng b√°n l·∫ª s√†n g·ªó, s√†n nh·ª±a, gi·∫•y d√°n t∆∞·ªùng, v·∫≠t li·ªáu n·ªôi th·∫•t ...\",\n",
    "    \"+85 s·∫£n ph·∫©m s√†n g·ªó Malaysia si√™u ch·ªãu n∆∞·ªõc gi√° t·ªët nh·∫•t S√†n g·ªó Malaysia ch√≠nh h√£ng, ch·∫•t l∆∞·ª£ng, gi√° c·∫£ c·∫°nh tranh ƒë∆∞·ª£c ph√¢n ph·ªëi b·ªüi JANHOME l√† h·ªá th·ªëng b√°n l·∫ª s√†n g·ªó, s√†n nh·ª±a, gi·∫•y d√°n t∆∞·ªùng, v·∫≠t li·ªáu n·ªôi th·∫•t ...\",\n",
    "    \"gi·ªçng_n·ªØ_tr·∫ßm\",\n",
    "    \"gi·ªçng_n·ªØ_tr·∫ßm-Ch·ªß t·ªãch HƒêQT B·ªánh vi·ªán th·∫©m m·ªπ Sao H√†n chia s·∫ª r·∫±ng, th·∫©m m·ªπ hay ph·∫´u thu·∫≠t th·∫©m m·ªπ th√¨ y√™u c·∫ßu, mong mu·ªën ƒë·∫ßu ti√™n ch·∫Øc ch·∫Øn ph·∫£i ƒë·∫πp.\",\n",
    "    \"gi·ªçng_n·ªØ_tr·∫ßm gi·ªçng_n·ªØ_tr·∫ßm-Ch·ªß t·ªãch HƒêQT B·ªánh vi·ªán th·∫©m m·ªπ Sao H√†n chia s·∫ª r·∫±ng, th·∫©m m·ªπ hay ph·∫´u thu·∫≠t th·∫©m m·ªπ th√¨ y√™u c·∫ßu, mong mu·ªën ƒë·∫ßu ti√™n ch·∫Øc ch·∫Øn ph·∫£i ƒë·∫πp.\",\n",
    "    \"H·ªá th·ªëng QLVBDH: Trang ch·ªß\",\n",
    "    \"ƒêƒÉng nh·∫≠p ƒêƒÉng nh·∫≠p. Chuy·ªÉn t·ªõi trang ƒë·∫ßy ƒë·ªß. ƒêƒÉng nh·∫≠p h·ªá th·ªëng. Ph√≠m chuy·ªÉn ch·ªØ hoa ƒëang b·∫≠t. ƒêƒÉng nh·∫≠p. ƒêƒÉng nh·∫≠p qua h·ªá th·ªëng x√°c th·ª±c TP. C·∫ßn Th∆°.\",\n",
    "    \"H·ªá th·ªëng QLVBDH: Trang ch·ªß ƒêƒÉng nh·∫≠p ƒêƒÉng nh·∫≠p. Chuy·ªÉn t·ªõi trang ƒë·∫ßy ƒë·ªß. ƒêƒÉng nh·∫≠p h·ªá th·ªëng. Ph√≠m chuy·ªÉn ch·ªØ hoa ƒëang b·∫≠t. ƒêƒÉng nh·∫≠p. ƒêƒÉng nh·∫≠p qua h·ªá th·ªëng x√°c th·ª±c TP. C·∫ßn Th∆°.\",\n",
    "    \"ƒêƒÉng nh·∫≠p VIC\",\n",
    "    \"T√™n ƒëƒÉng nh·∫≠p : M·∫≠t kh·∫©u : ƒêƒÉng nh·∫≠p. Tho√°t. VIC 6.5 ƒê∆∞·ª£c ph√°t tri·ªÉn b·ªüi c√¥ng ty CINOTEC 282 L√™ Quang ƒê·ªãnh, Ph∆∞·ªùng 11, Qu·∫≠n B√¨nh Th·∫°nh, TP HCM.\"\n",
    "    \"ƒêƒÉng nh·∫≠p VIC T√™n ƒëƒÉng nh·∫≠p : M·∫≠t kh·∫©u : ƒêƒÉng nh·∫≠p. Tho√°t. VIC 6.5 ƒê∆∞·ª£c ph√°t tri·ªÉn b·ªüi c√¥ng ty CINOTEC 282 L√™ Quang ƒê·ªãnh, Ph∆∞·ªùng 11, Qu·∫≠n B√¨nh Th·∫°nh, TP HCM.\"\n",
    "    \"ƒê·ªëi t∆∞·ª£ng b·∫£o tr·ª£\",\n",
    "    \"Trung t√¢m B·∫£o tr·ª£ X√£ h·ªôi l√† n∆°i qu·∫£n l√Ω chƒÉm s√≥c, nu√¥i d∆∞·ª°ng ƒëi·ªÅu tr·ªã ƒë·ªëi t∆∞·ª£ng B·∫£o tr·ª£ theo quy ƒë·ªãnh c·ªßa nh√† n∆∞·ªõc - ƒê·ªãa ch·ªâ: Khu v·ª±c B√¨nh H√≤a A, ...\",\n",
    "    \"ƒê·ªëi t∆∞·ª£ng b·∫£o tr·ª£ Trung t√¢m B·∫£o tr·ª£ X√£ h·ªôi l√† n∆°i qu·∫£n l√Ω chƒÉm s√≥c, nu√¥i d∆∞·ª°ng ƒëi·ªÅu tr·ªã ƒë·ªëi t∆∞·ª£ng B·∫£o tr·ª£ theo quy ƒë·ªãnh c·ªßa nh√† n∆∞·ªõc - ƒê·ªãa ch·ªâ: Khu v·ª±c B√¨nh H√≤a A, ...\",\n",
    "    \"ƒêƒÉng k√Ω doanh nghi·ªáp qua m·∫°ng ƒëi·ªán t·ª≠\",\n",
    "    \"H√¨nh I.1.2. Bi·ªÉu m·∫´u nh·∫≠p th√¥ng tin ƒëƒÉng k√Ω t√†i kho·∫£n. Qu√Ω doanh nghi·ªáp nh·∫≠p ƒë·∫ßy ƒë·ªß c√°c tr∆∞·ªùng th√¥ng tin c·ªßa bi·ªÉu m·∫´u theo H√¨nh.\",\n",
    "    \"ƒêƒÉng k√Ω doanh nghi·ªáp qua m·∫°ng ƒëi·ªán t·ª≠ H√¨nh I.1.2. Bi·ªÉu m·∫´u nh·∫≠p th√¥ng tin ƒëƒÉng k√Ω t√†i kho·∫£n. Qu√Ω doanh nghi·ªáp nh·∫≠p ƒë·∫ßy ƒë·ªß c√°c tr∆∞·ªùng th√¥ng tin c·ªßa bi·ªÉu m·∫´u theo H√¨nh.\",\n",
    "    \"ƒêƒÉng nh·∫≠p h·ªá th·ªëng\",\n",
    "    \"ƒêƒÉng nh·∫≠p h·ªá th·ªëng. L∆∞u t√†i kho·∫£n ƒëƒÉng nh·∫≠p. ƒêƒÉng nh·∫≠p. Qu√™n m·∫≠t kh·∫©u.\",\n",
    "\"ƒêƒÉng nh·∫≠p h·ªá th·ªëng ƒêƒÉng nh·∫≠p h·ªá th·ªëng. L∆∞u t√†i kho·∫£n ƒëƒÉng nh·∫≠p. ƒêƒÉng nh·∫≠p. Qu√™n m·∫≠t kh·∫©u.\",\n",
    "    \"Li√™n ƒëo√†n Taekwondo TP C·∫ßn Th∆° tƒÉng c∆∞·ªùng chuy·ªÉn ƒë·ªïi ...\",\n",
    "    \"M·ª•c ti√™u c·ªßa Li√™n ƒëo√†n Taekwondo TP C·∫ßn Th∆° trong nƒÉm 2024 l√† ƒë·∫©y m·∫°nh chuy·ªÉn ƒë·ªïi s·ªë, x√¢y d·ª±ng m√¥ h√¨nh qu·∫£n l√Ω ph√π h·ª£p ƒëi·ªÅu ki·ªán, xu th·∫ø v√† quy ƒë·ªãnh c·ªßa Li√™n ...\",\n",
    "    \"Li√™n ƒëo√†n Taekwondo TP C·∫ßn Th∆° tƒÉng c∆∞·ªùng chuy·ªÉn ƒë·ªïi ... M·ª•c ti√™u c·ªßa Li√™n ƒëo√†n Taekwondo TP C·∫ßn Th∆° trong nƒÉm 2024 l√† ƒë·∫©y m·∫°nh chuy·ªÉn ƒë·ªïi s·ªë, x√¢y d·ª±ng m√¥ h√¨nh qu·∫£n l√Ω ph√π h·ª£p ƒëi·ªÅu ki·ªán, xu th·∫ø v√† quy ƒë·ªãnh c·ªßa Li√™n ...\",\n",
    "    \"M·∫™U CHUY·ªÜN ‚Äúƒê√îI D√âP B√ÅC H·ªí‚Äù\",\n",
    "    \"3 thg 10, 2023 ‚Äî M·ªôt anh nhanh tay gi√†nh l·∫•y chi·∫øc d√©p, gi∆° l√™n nh∆∞ng ng·ªõ ra, l√∫ng t√∫ng. Anh b√™n c·∫°nh li·∫øc th·∫•y, ‚Äúv∆∞·ª£t v√¢y‚Äù ch·∫°y bi·∫øn‚Ä¶ B√°c ph·∫£i gi·ª•c:‚Äú∆† k√¨a, ng·∫Øm ...\",\n",
    "    \"M·∫™U CHUY·ªÜN ‚Äúƒê√îI D√âP B√ÅC H·ªí‚Äù 3 thg 10, 2023 ‚Äî M·ªôt anh nhanh tay gi√†nh l·∫•y chi·∫øc d√©p, gi∆° l√™n nh∆∞ng ng·ªõ ra, l√∫ng t√∫ng. Anh b√™n c·∫°nh li·∫øc th·∫•y, ‚Äúv∆∞·ª£t v√¢y‚Äù ch·∫°y bi·∫øn‚Ä¶ B√°c ph·∫£i gi·ª•c:‚Äú∆† k√¨a, ng·∫Øm ...\",\n",
    "    \"·∫¢nh h∆∞·ªüng c·ªßa c√°c lo·∫°i th·ª©c ƒÉn ƒë·∫øn sinh tr∆∞·ªüng v√† t·ªâ l·ªá ...\",\n",
    "    \"6 thg 12, 2022 ‚Äî l√† m·ªôt trong nh·ªØng lo√†i ch√¢n b·ª•ng n∆∞·ªõc ng·ªçt ƒë∆∞·ª£c t√¨m th·∫•y trong ao n∆∞·ªõc ng·ªçt, v≈©ng, b·ªÉ, h·ªì, ƒë·∫ßm l·∫ßy, ru·ªông l√∫a v√† ƒë√¥i khi ·ªü s√¥ng su·ªëi. Hi·ªán nay, ...\",\n",
    "    \"·∫¢nh h∆∞·ªüng c·ªßa c√°c lo·∫°i th·ª©c ƒÉn ƒë·∫øn sinh tr∆∞·ªüng v√† t·ªâ l·ªá ... 6 thg 12, 2022 ‚Äî l√† m·ªôt trong nh·ªØng lo√†i ch√¢n b·ª•ng n∆∞·ªõc ng·ªçt ƒë∆∞·ª£c t√¨m th·∫•y trong ao n∆∞·ªõc ng·ªçt, v≈©ng, b·ªÉ, h·ªì, ƒë·∫ßm l·∫ßy, ru·ªông l√∫a v√† ƒë√¥i khi ·ªü s√¥ng su·ªëi. Hi·ªán nay, ...\",\n",
    "    \"L·ªãch s·ª≠ h√¨nh th√†nh\",\n",
    "    \"Ng√†y 01 th√°ng 01 nƒÉm 2004, t·ªânh C·∫ßn Th∆° ƒë∆∞·ª£c chia t√°ch th√†nh 02 ƒë∆°n v·ªã h√†nh ch√≠nh l√† TP. C·∫ßn Th∆° v√† t·ªânh H·∫≠u Giang. B·∫£o t√†ng t·ªânh C·∫ßn ƒë·ªïi t√™n cho ph√π h·ª£p v·ªõi ...\",\n",
    "    \"L·ªãch s·ª≠ h√¨nh th√†nh Ng√†y 01 th√°ng 01 nƒÉm 2004, t·ªânh C·∫ßn Th∆° ƒë∆∞·ª£c chia t√°ch th√†nh 02 ƒë∆°n v·ªã h√†nh ch√≠nh l√† TP. C·∫ßn Th∆° v√† t·ªânh H·∫≠u Giang. B·∫£o t√†ng t·ªânh C·∫ßn ƒë·ªïi t√™n cho ph√π h·ª£p v·ªõi ...\",\n",
    "    \"H·ªôi C·ª±u chi·∫øn binh th√†nh ph·ªë C·∫ßn Th∆°\",\n",
    "    \"Th√¥ng tin li√™n h·ªá. H·ªôi C·ª±u chi·∫øn binh - Th√†nh ph·ªë C·∫ßn Th∆° ƒê·ªãa ch·ªâ : 22 Tr·∫ßn VƒÉn Ho√†i, P.Xu√¢n Kh√°nh, Q.Ninh Ki·ªÅu, TP C·∫ßn Th∆° ƒêi·ªán tho·∫°i: (0710) 3832735\",\n",
    "    \"H·ªôi C·ª±u chi·∫øn binh th√†nh ph·ªë C·∫ßn Th∆° Th√¥ng tin li√™n h·ªá. H·ªôi C·ª±u chi·∫øn binh - Th√†nh ph·ªë C·∫ßn Th∆° ƒê·ªãa ch·ªâ : 22 Tr·∫ßn VƒÉn Ho√†i, P.Xu√¢n Kh√°nh, Q.Ninh Ki·ªÅu, TP C·∫ßn Th∆° ƒêi·ªán tho·∫°i: (0710) 3832735\",\n",
    "    \"vƒÉn h√≥a ƒê·ªãa ƒëi·ªÉm Chi·∫øn th·∫Øng √îng ƒê∆∞a nƒÉm 1960\",\n",
    "\"DI T√çCH L·ªäCH S·ª¨ - VƒÇN H√ìA ƒê·ªäA ƒêI·ªÇM CHI·∫æN TH·∫ÆNG √îNG ƒê∆ØA NƒÇM 1960 ... Di t√≠ch l·ªãch s·ª≠ - vƒÉn h√≥a ƒê·ªãa ƒëi·ªÉm Chi·∫øn th·∫Øng √îng ƒê∆∞a nƒÉm 1960 t·ªça l·∫°c t·∫°i ·∫•p ƒê·ªãnh Kh√°nh A, ...\",\n",
    "    \"vƒÉn h√≥a ƒê·ªãa ƒëi·ªÉm Chi·∫øn th·∫Øng √îng ƒê∆∞a nƒÉm 1960 DI T√çCH L·ªäCH S·ª¨ - VƒÇN H√ìA ƒê·ªäA ƒêI·ªÇM CHI·∫æN TH·∫ÆNG √îNG ƒê∆ØA NƒÇM 1960 ... Di t√≠ch l·ªãch s·ª≠ - vƒÉn h√≥a ƒê·ªãa ƒëi·ªÉm Chi·∫øn th·∫Øng √îng ƒê∆∞a nƒÉm 1960 t·ªça l·∫°c t·∫°i ·∫•p ƒê·ªãnh Kh√°nh A, ...\",\n",
    "    \"∆Ø·ªõc ao c·ªßa thi·∫øu nhi qua b√†i h√°t\",\n",
    "    \"∆Ø·ªöC AO C·ª¶A THI·∫æU NHI QUA B√ÄI H√ÅT ‚ÄúEM M∆† G·∫∂P B√ÅC H·ªí‚Äù C·ª¶A NH·∫†C Sƒ® XU√ÇN GIAO. Nh·∫°c sƒ© Xu√¢n Giao qu√™ g·ªëc ·ªü Nh∆∞ Qu·ª≥nh, VƒÉn L√¢m, H∆∞ng Y√™n, sinh nƒÉm 1932 t·∫°i H·∫£i ...\",\n",
    "    \"∆Ø·ªõc ao c·ªßa thi·∫øu nhi qua b√†i h√°t ∆Ø·ªöC AO C·ª¶A THI·∫æU NHI QUA B√ÄI H√ÅT ‚ÄúEM M∆† G·∫∂P B√ÅC H·ªí‚Äù C·ª¶A NH·∫†C Sƒ® XU√ÇN GIAO. Nh·∫°c sƒ© Xu√¢n Giao qu√™ g·ªëc ·ªü Nh∆∞ Qu·ª≥nh, VƒÉn L√¢m, H∆∞ng Y√™n, sinh nƒÉm 1932 t·∫°i H·∫£i ...\",\n",
    "    \"B·∫¢NG THANH TO√ÅN PH·ª§ C·∫§P C√ÅN B·ªò C√îNG ƒêO√ÄN\",\n",
    "    \"B·∫¢NG THANH TO√ÅN PH·ª§ C·∫§P C√ÅN B·ªò C√îNG ƒêO√ÄN ¬∑ C√°c bi·ªÉu m·∫´u t√†i ch√≠nh C√¥ng ƒëo√†n c∆° s·ªü ¬∑ M·∫´u h∆∞·ªõng d·∫´n C√¥ng ƒëo√†n c∆° s·ªü ¬∑ M·∫´u bi·ªÉu d·ª± to√°n, quy·∫øt to√°n t√†i ch√≠nh Cƒê ...\",\n",
    "    \"B·∫¢NG THANH TO√ÅN PH·ª§ C·∫§P C√ÅN B·ªò C√îNG ƒêO√ÄN B·∫¢NG THANH TO√ÅN PH·ª§ C·∫§P C√ÅN B·ªò C√îNG ƒêO√ÄN ¬∑ C√°c bi·ªÉu m·∫´u t√†i ch√≠nh C√¥ng ƒëo√†n c∆° s·ªü ¬∑ M·∫´u h∆∞·ªõng d·∫´n C√¥ng ƒëo√†n c∆° s·ªü ¬∑ M·∫´u bi·ªÉu d·ª± to√°n, quy·∫øt to√°n t√†i ch√≠nh Cƒê ...\",\n",
    "    \"dung tin co ay tap 2 S√≤ng b·∫°c th√¥ng th∆∞·ªùng c·ªßa Vi·ªát Nam\",\n",
    "    \"dung tin co ay tap 2 -Xu√°¬∫¬•t hi√°¬ª‚Ä°n c√É¬πng ki√°¬ª∆íu t√É¬≥c layer vu√°¬ª't ng√Ü¬∞√°¬ª¬£c v√°¬ª‚Ä∫i ph√°¬∫¬ßn t√É¬≥c t√°¬ª¬´ hai mang tai √Ñ'√°¬ª u √Ñ'√Ü¬∞√°¬ª¬£c h√°¬∫¬•t ng√Ü¬∞√°¬ª¬£c ra sau v√É ...\",\n",
    "    \"dung tin co ay tap 2 S√≤ng b·∫°c th√¥ng th∆∞·ªùng c·ªßa Vi·ªát Nam dung tin co ay tap 2 -Xu√°¬∫¬•t hi√°¬ª‚Ä°n c√É¬πng ki√°¬ª∆íu t√É¬≥c layer vu√°¬ª't ng√Ü¬∞√°¬ª¬£c v√°¬ª‚Ä∫i ph√°¬∫¬ßn t√É¬≥c t√°¬ª¬´ hai mang tai √Ñ'√°¬ª u √Ñ'√Ü¬∞√°¬ª¬£c h√°¬∫¬•t ng√Ü¬∞√°¬ª¬£c ra sau v√É ...\",\n",
    "    \"Th√¥ng tin truy n√£, ƒë√¨nh n√£\",\n",
    "    \"Th√¥ng tin truy n√£, ƒë√¨nh n√£ ¬∑ Th√¥ng b√°o ¬∑ Th√¥ng tin truy n√£ ¬∑ Li√™n k·∫øt webiste ¬∑ ThƒÉm d√≤ √Ω ki·∫øn ¬∑ S·ªë l∆∞·ª£t truy c·∫≠p. Trong ng√†y: T·∫•t c·∫£:.\",\"C·ªù b·∫°c\"\n",
    "    \"Th√¥ng tin truy n√£, ƒë√¨nh n√£ Th√¥ng tin truy n√£, ƒë√¨nh n√£ ¬∑ Th√¥ng b√°o ¬∑ Th√¥ng tin truy n√£ ¬∑ Li√™n k·∫øt webiste ¬∑ ThƒÉm d√≤ √Ω ki·∫øn ¬∑ S·ªë l∆∞·ª£t truy c·∫≠p. Trong ng√†y: T·∫•t c·∫£:.\",\"C·ªù b·∫°c\"\n",
    "    \"+79 s·∫£n ph·∫©m s√†n g·ªó Florton ch√≠nh h√£ng, ch·∫•t l∆∞·ª£ng, gi√° r·∫ª\",\n",
    "    \"S√ÄN G·ªñ FLORTON ch·∫•t l∆∞·ª£ng, gi√° r·∫ª, ƒë·∫°t ti√™u chu·∫©n Ch√¢u √Çu ƒë∆∞·ª£c cung c·∫•p b·ªüi JANHOME l√† h·ªá th·ªëng b√°n h√†ng t·∫°i kho cung c·∫•p v·∫≠t li·ªáu s√†n g·ªó gi·∫•y d√°n t∆∞·ªùng ...\",\n",
    "\"+79 s·∫£n ph·∫©m s√†n g·ªó Florton ch√≠nh h√£ng, ch·∫•t l∆∞·ª£ng, gi√° r·∫ª S√ÄN G·ªñ FLORTON ch·∫•t l∆∞·ª£ng, gi√° r·∫ª, ƒë·∫°t ti√™u chu·∫©n Ch√¢u √Çu ƒë∆∞·ª£c cung c·∫•p b·ªüi JANHOME l√† h·ªá th·ªëng b√°n h√†ng t·∫°i kho cung c·∫•p v·∫≠t li·ªáu s√†n g·ªó gi·∫•y d√°n t∆∞·ªùng ...\",\n",
    "    \"Trang ch·ªß - C·∫ßn Th∆°\",\n",
    "    \"B·ªô C√¥ng Th∆∞∆°ng v·ª´a ban h√†nh Th√¥ng t∆∞ quy ƒë·ªãnh vi·ªác nh·∫≠p kh·∫©u m·∫∑t h√†ng g·∫°o v√† l√° thu·ªëc l√° kh√¥ c√≥ xu·∫•t x·ª© t·ª´ Campuchia theo h·∫°n ng·∫°ch thu·∫ø quan nƒÉm 2023 v√† 2024.\",\n",
    "    \"Trang ch·ªß - C·∫ßn Th∆° B·ªô C√¥ng Th∆∞∆°ng v·ª´a ban h√†nh Th√¥ng t∆∞ quy ƒë·ªãnh vi·ªác nh·∫≠p kh·∫©u m·∫∑t h√†ng g·∫°o v√† l√° thu·ªëc l√° kh√¥ c√≥ xu·∫•t x·ª© t·ª´ Campuchia theo h·∫°n ng·∫°ch thu·∫ø quan nƒÉm 2023 v√† 2024.\",\n",
    "    \"Login\",\n",
    "    \"???login.label.loginheading.left??? ???login.label.userid??? ???login.label.password??? Help. Product documentation ¬∑ Product wiki ¬∑ Media gallery ...\",\n",
    "    \"Login ???login.label.loginheading.left??? ???login.label.userid??? ???login.label.password??? Help. Product documentation ¬∑ Product wiki ¬∑ Media gallery ...\",\n",
    "    \"ƒêƒÉng k√Ω t∆∞ v·∫•n t·ª´ QR code\",\n",
    "    \"ƒêƒÉng k√Ω t∆∞ v·∫•n t·ª´ QR code. Xo√° D√°n (Paste)\",\n",
    "    \"ƒêƒÉng k√Ω t∆∞ v·∫•n t·ª´ QR code ƒêƒÉng k√Ω t∆∞ v·∫•n t·ª´ QR code. Xo√° D√°n (Paste)\",\n",
    "    \"GGBET\",\n",
    "    \"ggbet\",\n",
    "    \"+79 s·∫£n ph·∫©m s√†n g·ªó Florton ch√≠nh h√£ng, ch·∫•t l∆∞·ª£ng, gi√° r·∫ª\",\n",
    "    \"S√ÄN G·ªñ FLORTON ch·∫•t l∆∞·ª£ng, gi√° r·∫ª, ƒë·∫°t ti√™u chu·∫©n Ch√¢u √Çu ƒë∆∞·ª£c cung c·∫•p b·ªüi JANHOME l√† h·ªá th·ªëng b√°n h√†ng t·∫°i kho cung c·∫•p v·∫≠t li·ªáu s√†n g·ªó gi·∫•y d√°n t∆∞·ªùng ...\",\n",
    "    \"+79 s·∫£n ph·∫©m s√†n g·ªó Florton ch√≠nh h√£ng, ch·∫•t l∆∞·ª£ng, gi√° r·∫ª S√ÄN G·ªñ FLORTON ch·∫•t l∆∞·ª£ng, gi√° r·∫ª, ƒë·∫°t ti√™u chu·∫©n Ch√¢u √Çu ƒë∆∞·ª£c cung c·∫•p b·ªüi JANHOME l√† h·ªá th·ªëng b√°n h√†ng t·∫°i kho cung c·∫•p v·∫≠t li·ªáu s√†n g·ªó gi·∫•y d√°n t∆∞·ªùng ...\",\n",
    "    \"k√™nh-xoilac\",\n",
    "    \"k√™nh-xoilac-Ng√†y 3.1, HƒêND H.Qu·∫ø S∆°n kh√≥a XII (nhi·ªám k·ª≥ 2021 - 2026) t·ªï ch·ª©c k·ª≥ h·ªçp chuy√™n ƒë·ªÅ th·ª© 15 ƒë·ªÉ b·∫ßu c√°c ch·ª©c danh l√£nh ƒë·∫°o ch·ªß ch·ªët sau s√°p nh·∫≠p ...\",\n",
    "    \"k√™nh-xoilac k√™nh-xoilac-Ng√†y 3.1, HƒêND H.Qu·∫ø S∆°n kh√≥a XII (nhi·ªám k·ª≥ 2021 - 2026) t·ªï ch·ª©c k·ª≥ h·ªçp chuy√™n ƒë·ªÅ th·ª© 15 ƒë·ªÉ b·∫ßu c√°c ch·ª©c danh l√£nh ƒë·∫°o ch·ªß ch·ªët sau s√°p nh·∫≠p ...\",\n",
    "    \"c√°ch lai t·∫°o m√†u l√¥ng g√† 0209\",\n",
    "    \"c√°ch lai t·∫°o m√†u l√¥ng g√† 0209-K·∫ø ti·∫øp l√† cung ƒëi·ªán Minos t·ªça l·∫°c ·ªü Crete, (Hy L·∫°p), ƒë∆∞·ª£c cho l√† x√¢y d·ª±ng v√†o kho·∫£ng nƒÉm 1700 tr∆∞·ªõc C√¥ng nguy√™n, ...\",\n",
    "    \"c√°ch lai t·∫°o m√†u l√¥ng g√† 0209 c√°ch lai t·∫°o m√†u l√¥ng g√† 0209-K·∫ø ti·∫øp l√† cung ƒëi·ªán Minos t·ªça l·∫°c ·ªü Crete, (Hy L·∫°p), ƒë∆∞·ª£c cho l√† x√¢y d·ª±ng v√†o kho·∫£ng nƒÉm 1700 tr∆∞·ªõc C√¥ng nguy√™n, ...\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(example_texts):\n",
    "    predicted_label = predict_text_class(text)\n",
    "\n",
    "    print(f\"  Input: '{text}'\")\n",
    "    print(f\"  Predicted: {predicted_label}\")\n",
    "    print()\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for i, text in enumerate(example_texts):\n",
    "#     predicted_label = predict_text_class(text)\n",
    "#     results.append(f\"Input: '{text}'\\nPredicted: {predicted_label}\\n\")\n",
    "\n",
    "# # Save to file\n",
    "# with open(\"predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"\\n\".join(results))\n",
    "\n",
    "# print(\"Predictions saved to predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3413eec2-e856-478a-a139-f12846bf3fb4",
    "_uuid": "031b3fe1-7a31-49ac-b82b-147b9f5a420a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "65150f08-afb6-4b3d-89a5-d734af8e7134",
    "_uuid": "208d9ea6-0583-4e3e-9e86-26965efeccf2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7954816,
     "sourceId": 12881853,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

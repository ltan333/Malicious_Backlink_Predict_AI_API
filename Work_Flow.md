#  Malicious Backlink Prediction â€” **Kaggle Edition (.ipynb)**

This repo uses **Jupyter notebooks (.ipynb)** on **Kaggle** for both **training** and **inference**.  

> **Training notebook:** `training.ipynb`  
> **Inference notebook:** `inference.ipynb`  
> **Author:** Huá»³nh Ngá»c NhÆ° Quá»³nh  
> **Last updated:** 2025-10-17 08:23

---

## âœ¨ Highlights

- 15 classes: `gambling, movies, ecommerce, government, education, technology, tourism, health, finance, media, nonprofit, realestate, services, industries, agriculture`
- Vietnamese word segmentation via **py_vncorenlp**
- Hugging Face **Trainer** + **custom Focal Loss (Î³=2.0)** + **label smoothing (0.2)** + **class weights**
- Token length diagnostics, stratified split, eval metrics (accuracy / precision / recall / weighted F1)
- Misclassified samples exported to CSV
- Inference notebook supports **single text** and **batch CSV**

---

## ğŸ“ Repository Structure

```text
repo/
â”œâ”€ README.md
â”œâ”€ notebooks/
â”‚  â”œâ”€ training.ipynb
â”‚  â””â”€ inference.ipynb
â”œâ”€ model/                                        
â”œâ”€ dataset/
â””â”€  â””â”€ dataset.csv
```

---

## ğŸ—ƒï¸ Data & Labels

- **Columns required:**  
  - `text` â€” raw Vietnamese string  
  - `label` â€” one of 15 class names

- **Label mapping (0â€“14):**
```python
{
  "gambling":0, "movies":1, "ecommerce":2, "government":3, "education":4,
  "technology":5, "tourism":6, "health":7, "finance":8, "media":9,
  "nonprofit":10, "realestate":11, "services":12, "industries":13, "agriculture":14
}
```

---

## ğŸ““ Training Notebook (`training.ipynb`)
1. **Install & imports** (`%pip install ...`, `import torch`, `transformers`, `py_vncorenlp`, etc.)
2. **Reproducibility** â€” set global seed `42`
3. **VnCoreNLP** â€” download or load the model, init segmenter
4. **Load CSV to HF Dataset** â€” map labels â†’ IDs
5. **Preprocess** â€” word-seg â†’ PhoBERT tokenize (`max_length=64`, truncation & pad)
6. **Train/val split** â€” stratified ~90/10 (quick sanity-check eval)
7. **Class weights** â€” `compute_class_weight('balanced')`
8. **Loss/Trainer** â€” custom **Focal Loss (Î³=2.0, label_smoothing=0.2)**; `TrainingArguments`
9. **Train** â€” epochs=20, batch=32, lr=2e-5, wd=0.01, warmup=500, early stopping (patience=5, threshold=0.001)
10. **Save artifacts** â€” model & tokenizer to: `/kaggle/working/fine_tuned_phobert/`
11. **Evaluate** â€” print metrics; export `misclassified_samples.csv` to `/kaggle/working/`
12. *(Optional)* **Plots** â€” loss/metrics curves (cells can be toggled on)

---

## ğŸ§ª â€œDataâ€‘Maxâ€ Mode (optional)

**Goal:** train on **100%** labeled data; evaluate on a **small stratified subset** from the same data for a quick health check.

- Use **only** for the final fit after hyperparameters are chosen on a proper split or kâ€‘fold CV.
- Metrics are **optimistic** (data leakage) â€” document clearly.

**Notebook hint:** set
```python
train_dataset = tokenized_dataset
eval_dataset  = tokenized_dataset.select(eval_indices)  # small stratified subset
```
---

## ğŸ““ Inference Notebook (`inference.ipynb`)

---